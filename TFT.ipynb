{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:38.947433Z",
     "start_time": "2024-03-09T19:49:34.658375Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.pipeline import Pipeline\n",
    "from darts.models import TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer, MissingValuesFiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:38.950949Z",
     "start_time": "2024-03-09T19:49:38.948497Z"
    }
   },
   "outputs": [],
   "source": [
    "TIME_COL = \"Date\"\n",
    "TARGET = \"Weekly_Sales\"\n",
    "STATIC_COV = [\"Store\", \"Dept\", \"Type\", \"Size\"]\n",
    "DYNAMIC_COV_FILL_0 = [\"IsHoliday\", 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "DYNAMIC_COV_FILL_INTERPOLATE = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "FREQ = \"W-FRI\"\n",
    "FORECAST_HORIZON = 16 # weeks\n",
    "SCALER = Scaler()\n",
    "TRANSFORMER = StaticCovariatesTransformer()\n",
    "PIPELINE = Pipeline([SCALER, TRANSFORMER])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:39.245985Z",
     "start_time": "2024-03-09T19:49:38.952348Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data and exogenous features\n",
    "df = pd.read_csv('data/train.csv')\n",
    "store_info = pd.read_csv('data/stores.csv')\n",
    "exo_feat = pd.read_csv('data/features.csv').drop(columns='IsHoliday')\n",
    "\n",
    "# join all data frames\n",
    "df = pd.merge(df, store_info, on=['Store'], how='left')\n",
    "df = pd.merge(df, exo_feat, on=['Store', TIME_COL], how='left')\n",
    "\n",
    "# process data\n",
    "df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    "df[TARGET] = np.where(df[TARGET] < 0, 0, df[TARGET]) # remove negative values\n",
    "df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5']] = df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5']].fillna(0) # fill missing values with nan\n",
    "df[\"IsHoliday\"] = df[\"IsHoliday\"]*1 # convert boolean into binary\n",
    "df[\"Size\"] = np.where(df[\"Size\"] < store_info[\"Size\"].quantile(0.25), \"small\",\n",
    "                np.where(df[\"Size\"] > store_info[\"Size\"].quantile(0.75), \"large\",\n",
    "                \"medium\")) # make size a categorical variable\n",
    "\n",
    "# reduce running time by forecasting only top 7 stores\n",
    "top_7_stores = df.groupby(['Store']).agg({TARGET: 'sum'}).reset_index().sort_values(by=TARGET, ascending=False).head(7)\n",
    "df = df[df['Store'].isin(top_7_stores['Store'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:39.435061Z",
     "start_time": "2024-03-09T19:49:39.248345Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 weeks to for test\n",
    "train = df[df[TIME_COL] <= (max(df[TIME_COL])-timedelta(weeks=FORECAST_HORIZON))]\n",
    "test = df[df[TIME_COL] > (max(df[TIME_COL])-timedelta(weeks=FORECAST_HORIZON))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranform data into Darts format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:43.397514Z",
     "start_time": "2024-03-09T19:49:39.435659Z"
    }
   },
   "outputs": [],
   "source": [
    "# read train and test datasets and transform train dataset\n",
    "train_darts = TimeSeries.from_group_dataframe(df=train, group_cols=STATIC_COV, time_col=TIME_COL, value_cols=TARGET, freq=FREQ, fill_missing_dates=True, fillna_value=0)\n",
    "test_darts = TimeSeries.from_group_dataframe(df=df, group_cols=STATIC_COV, time_col=TIME_COL, value_cols=TARGET, freq=FREQ, fill_missing_dates=True, fillna_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dynamic Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:44.355884Z",
     "start_time": "2024-03-09T19:49:43.398235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ValueError: When concatenating along component or sample dimensions, all the series must have the same time axes (unless `ignore_time_axis` is True), or time axes of same lengths (if `ignore_time_axis` is True), and all series must have the same number of samples (if concatenating along component dimension), or the same number of components (if concatenating along sample dimension).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When concatenating along component or sample dimensions, all the series must have the same time axes (unless `ignore_time_axis` is True), or time axes of same lengths (if `ignore_time_axis` is True), and all series must have the same number of samples (if concatenating along component dimension), or the same number of components (if concatenating along sample dimension).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m dept \u001B[38;5;241m=\u001B[39m serie\u001B[38;5;241m.\u001B[39mstatic_covariates[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDept\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# create covariates to fill with 0\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m covariate \u001B[38;5;241m=\u001B[39m covariate\u001B[38;5;241m.\u001B[39mstack(\n\u001B[1;32m     27\u001B[0m             TimeSeries\u001B[38;5;241m.\u001B[39mfrom_dataframe(df[(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStore\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m store) \u001B[38;5;241m&\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDept\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m dept)], time_col\u001B[38;5;241m=\u001B[39mTIME_COL, value_cols\u001B[38;5;241m=\u001B[39mDYNAMIC_COV_FILL_0, freq\u001B[38;5;241m=\u001B[39mFREQ, fill_missing_dates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, fillna_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     28\u001B[0m         )\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# create covariates to fill with interpolation\u001B[39;00m\n\u001B[1;32m     31\u001B[0m dyn_cov_interp \u001B[38;5;241m=\u001B[39m TimeSeries\u001B[38;5;241m.\u001B[39mfrom_dataframe(df[(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStore\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m store) \u001B[38;5;241m&\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDept\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m dept)], time_col\u001B[38;5;241m=\u001B[39mTIME_COL, value_cols\u001B[38;5;241m=\u001B[39mDYNAMIC_COV_FILL_INTERPOLATE, freq\u001B[38;5;241m=\u001B[39mFREQ, fill_missing_dates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ZAAI/lib/python3.11/site-packages/darts/timeseries.py:2964\u001B[0m, in \u001B[0;36mTimeSeries.stack\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   2944\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstack\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTimeSeries\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTimeSeries\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2945\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2946\u001B[0m \u001B[38;5;124;03m    Stacks another univariate or multivariate TimeSeries with the same time index on top of\u001B[39;00m\n\u001B[1;32m   2947\u001B[0m \u001B[38;5;124;03m    the current one (along the component axis).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m        A new multivariate TimeSeries instance.\u001B[39;00m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m concatenate([\u001B[38;5;28mself\u001B[39m, other], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ZAAI/lib/python3.11/site-packages/darts/timeseries.py:5285\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(series, axis, ignore_time_axis, ignore_static_covariates, drop_hierarchy)\u001B[0m\n\u001B[1;32m   5272\u001B[0m time_axes_equal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m   5273\u001B[0m     \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m   5274\u001B[0m         \u001B[38;5;28mmap\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5277\u001B[0m     )\n\u001B[1;32m   5278\u001B[0m )\n\u001B[1;32m   5279\u001B[0m time_axes_ok \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   5280\u001B[0m     time_axes_equal\n\u001B[1;32m   5281\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ignore_time_axis\n\u001B[1;32m   5282\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlen\u001B[39m({\u001B[38;5;28mlen\u001B[39m(ts) \u001B[38;5;28;01mfor\u001B[39;00m ts \u001B[38;5;129;01min\u001B[39;00m series}) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   5283\u001B[0m )\n\u001B[0;32m-> 5285\u001B[0m raise_if_not(\n\u001B[1;32m   5286\u001B[0m     (\n\u001B[1;32m   5287\u001B[0m         time_axes_ok\n\u001B[1;32m   5288\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   5289\u001B[0m             (axis \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m sample_axis_equal)\n\u001B[1;32m   5290\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m (axis \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m component_axis_equal)\n\u001B[1;32m   5291\u001B[0m         )\n\u001B[1;32m   5292\u001B[0m     ),\n\u001B[1;32m   5293\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen concatenating along component or sample dimensions, all the series must have the same time \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   5294\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maxes (unless `ignore_time_axis` is True), or time axes of same lengths (if `ignore_time_axis` is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   5295\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrue), and all series must have the same number of samples (if concatenating along component \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   5296\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimension), or the same number of components (if concatenating along sample dimension).\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5297\u001B[0m )\n\u001B[1;32m   5299\u001B[0m \u001B[38;5;66;03m# we concatenate raw values using Numpy because not all series might have the same time axes\u001B[39;00m\n\u001B[1;32m   5300\u001B[0m \u001B[38;5;66;03m# and joining using xarray.concatenate() won't work in some cases\u001B[39;00m\n\u001B[1;32m   5301\u001B[0m concat_vals \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([da\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;28;01mfor\u001B[39;00m da \u001B[38;5;129;01min\u001B[39;00m da_sequence], axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ZAAI/lib/python3.11/site-packages/darts/logging.py:78\u001B[0m, in \u001B[0;36mraise_if_not\u001B[0;34m(condition, message, logger)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m condition:\n\u001B[1;32m     77\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValueError: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m message)\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[0;31mValueError\u001B[0m: When concatenating along component or sample dimensions, all the series must have the same time axes (unless `ignore_time_axis` is True), or time axes of same lengths (if `ignore_time_axis` is True), and all series must have the same number of samples (if concatenating along component dimension), or the same number of components (if concatenating along sample dimension)."
     ]
    }
   ],
   "source": [
    "# create dynamic covariates for each serie in the training darts\n",
    "dynamic_covariates = []\n",
    "for serie in train_darts:\n",
    "    # add the month and week as a covariate\n",
    "    covariate = datetime_attribute_timeseries(\n",
    "        serie,\n",
    "        attribute=\"month\",\n",
    "        one_hot=True,\n",
    "        cyclic=False,\n",
    "        add_length=FORECAST_HORIZON,\n",
    "    )\n",
    "    covariate = covariate.stack(\n",
    "        datetime_attribute_timeseries(\n",
    "            serie,\n",
    "            attribute=\"week\",\n",
    "            one_hot=True,\n",
    "            cyclic=False,\n",
    "            add_length=FORECAST_HORIZON,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    store = serie.static_covariates['Store'].item()\n",
    "    dept = serie.static_covariates['Dept'].item()\n",
    "\n",
    "    # create covariates to fill with 0\n",
    "    covariate = covariate.stack(\n",
    "                TimeSeries.from_dataframe(df[(df['Store'] == store) & (df['Dept'] == dept)], time_col=TIME_COL, value_cols=DYNAMIC_COV_FILL_0, freq=FREQ, fill_missing_dates=True, fillna_value=0)\n",
    "            )\n",
    "    \n",
    "    # create covariates to fill with interpolation\n",
    "    dyn_cov_interp = TimeSeries.from_dataframe(df[(df['Store'] == store) & (df['Dept'] == dept)], time_col=TIME_COL, value_cols=DYNAMIC_COV_FILL_INTERPOLATE, freq=FREQ, fill_missing_dates=True)\n",
    "    covariate = covariate.stack(MissingValuesFiller().transform(dyn_cov_interp))\n",
    "\n",
    "    dynamic_covariates.append(covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale covariates\n",
    "dynamic_covariates_transformed = SCALER.fit_transform(dynamic_covariates)\n",
    "\n",
    "# scale data and transform static covariates\n",
    "data_transformed = PIPELINE.fit_transform(train_darts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T19:49:44.359695Z",
     "start_time": "2024-03-09T19:49:44.359605Z"
    }
   },
   "outputs": [],
   "source": [
    "TFT_params = {\n",
    "    \"input_chunk_length\": 52, # number of weeks to lookback\n",
    "    \"output_chunk_length\": FORECAST_HORIZON,\n",
    "    \"hidden_size\": 2,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"num_attention_heads\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_epochs\": 3,\n",
    "    \"likelihood\": QuantileRegression(quantiles=[0.25, 0.5, 0.75]),\n",
    "    \"random_state\": 42,\n",
    "    \"use_static_covariates\": True,\n",
    "    \"optimizer_kwargs\": {\"lr\": 1e-3},\n",
    "}\n",
    "\n",
    "tft_model = TFTModel(**TFT_params)\n",
    "tft_model.fit(data_transformed, future_covariates=dynamic_covariates_transformed, verbose=False)\n",
    "pred = PIPELINE.inverse_transform(tft_model.predict(n=FORECAST_HORIZON, series=data_transformed, num_samples=50, future_covariates=dynamic_covariates_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.explainability import TFTExplainer\n",
    "\n",
    "explainer = TFTExplainer(\n",
    "    tft_model,\n",
    "    background_series=data_transformed[1],\n",
    "    background_future_covariates=dynamic_covariates_transformed[1],\n",
    ")\n",
    "explainability_result = explainer.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.barh(data=explainer._encoder_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Encoder Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.barh(data=explainer._decoder_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Decoder Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Cov Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.barh(data=explainer._static_covariates_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Static Cov Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.plot_attention(explainability_result, plot_type=\"all\", show_index_as='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mape, rmse\n",
    "\n",
    "def eval_model(val_series, pred_series):\n",
    "\n",
    "    # plot actual series\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    val_series[: pred_series.end_time()].plot(label=\"actual\")\n",
    "\n",
    "    # plot prediction with quantile ranges\n",
    "    pred_series.plot(\n",
    "        low_quantile=0.25, high_quantile=0.75, label=f\"{int(0.25 * 100)}-{int(0.75 * 100)}th percentiles\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"MAPE: {round(mape(val_series, pred_series),2)}% | RMSE: {round(rmse(val_series, pred_series),2)}\")\n",
    "    plt.legend()\n",
    "\n",
    "eval_model(test_darts[1], pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
